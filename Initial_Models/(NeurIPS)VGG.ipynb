{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjljNWlncFMTXKM9iI4z0O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pdf2image\n","!apt-get install -y poppler-utils"],"metadata":{"id":"h1bQ1dUbvJ4B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747653078753,"user_tz":-540,"elapsed":29549,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"4eee8c9f-1df0-4637-9201-dad183dfb932"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdf2image\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n","Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Installing collected packages: pdf2image\n","Successfully installed pdf2image-1.17.0\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n","Fetched 186 kB in 1s (223 kB/s)\n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 126102 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qn9p2CCAMtg7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747653096885,"user_tz":-540,"elapsed":15695,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"46e5c516-740d-479f-b414-9388ddb78780"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab import files\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from pdf2image import convert_from_path\n","import torchvision.transforms as transforms\n","\n","class FilenameLabelDataset(Dataset):\n","    def __init__(self, folder_path, transform=None):\n","        self.folder_path = folder_path\n","        self.transform = transform\n","        self.samples = []\n","\n","        for file in os.listdir(folder_path):\n","            if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n","                label = 0 if \"Accepted\" in file else 1\n","                self.samples.append((os.path.join(folder_path, file), label))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.samples[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","train_path = \"/content/drive/MyDrive/SAFE_AI(Project_proposal)/NeurIPS_Dataset_LowRes/train\"\n","test_path = \"/content/drive/MyDrive/SAFE_AI(Project_proposal)/NeurIPS_Dataset_LowRes/test\"\n","\n","train_dataset = FilenameLabelDataset(train_path, transform=transform)\n","test_dataset = FilenameLabelDataset(test_path, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)"],"metadata":{"id":"jdCmkPXnM2wW","executionInfo":{"status":"ok","timestamp":1747653116127,"user_tz":-540,"elapsed":16708,"user":{"displayName":"룰라비","userId":"00641962345652782370"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class VGG16Model(nn.Module):    # applying VGG16 model\n","    def __init__(self):\n","        super(VGG16Model, self).__init__()\n","        base_model = models.vgg16(pretrained=True)\n","\n","        self.conv = base_model.features\n","        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # set max pooling output by 7x7\n","\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512 * 7 * 7, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2)  # good or bad paper -> binary classification\n","        )\n","\n","        for param in self.conv.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.pool(x)\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"qF76XnmUM9Yn","executionInfo":{"status":"ok","timestamp":1747653121000,"user_tz":-540,"elapsed":9,"user":{"displayName":"룰라비","userId":"00641962345652782370"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG16Model().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)  # low IR is recommended for VGG\n","\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    train_acc = 100 * correct / total\n","    print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\")"],"metadata":{"id":"yTN2qr_eM_hE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e197af17-fc16-4be8-8a48-57d3cc42ab59","executionInfo":{"status":"ok","timestamp":1747655209568,"user_tz":-540,"elapsed":2085654,"user":{"displayName":"룰라비","userId":"00641962345652782370"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:08<00:00, 63.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1] Loss: 0.6021, Accuracy: 79.13%\n","[Epoch 2] Loss: 0.2193, Accuracy: 91.82%\n","[Epoch 3] Loss: 0.1289, Accuracy: 96.16%\n","[Epoch 4] Loss: 0.0751, Accuracy: 98.50%\n","[Epoch 5] Loss: 0.0458, Accuracy: 99.50%\n"]}]},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","test_acc = 100 * correct / total\n","print(f\"테스트 정확도: {test_acc:.2f}%\")"],"metadata":{"id":"NWsYOBOlNA5_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747655478155,"user_tz":-540,"elapsed":170008,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"09d795a5-7dae-4bf4-8a4e-426f0f287b3b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["테스트 정확도: 92.67%\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/SAFE_AI(Project_proposal)/Models/NeurIPS_vgg_model.pt\")"],"metadata":{"id":"LAWmJdtjNCC1","executionInfo":{"status":"ok","timestamp":1747655529830,"user_tz":-540,"elapsed":247,"user":{"displayName":"룰라비","userId":"00641962345652782370"}}},"execution_count":7,"outputs":[]}]}
