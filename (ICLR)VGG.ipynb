{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install pdf2image\n","!apt-get install -y poppler-utils"],"metadata":{"id":"h1bQ1dUbvJ4B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747650280906,"user_tz":-540,"elapsed":7196,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"0f5743a8-2ca0-40ba-ef33-3352d4d68247"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n","0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn9p2CCAMtg7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747650289467,"user_tz":-540,"elapsed":2156,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"1d5cc2a5-6940-4380-b26b-c18789847e63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab import files\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from pdf2image import convert_from_path\n","import torchvision.transforms as transforms\n","\n","class FilenameLabelDataset(Dataset):\n","    def __init__(self, folder_path, transform=None):\n","        self.folder_path = folder_path\n","        self.transform = transform\n","        self.samples = []\n","\n","        for file in os.listdir(folder_path):\n","            if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n","                label = 0 if \"Accepted\" in file else 1\n","                self.samples.append((os.path.join(folder_path, file), label))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.samples[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","train_path = \"/content/drive/MyDrive/SAFE_AI(Project_proposal)/ICLR_Dataset_LowRes/train\"\n","test_path = \"/content/drive/MyDrive/SAFE_AI(Project_proposal)/ICLR_Dataset_LowRes/test\"\n","\n","train_dataset = FilenameLabelDataset(train_path, transform=transform)\n","test_dataset = FilenameLabelDataset(test_path, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)"],"metadata":{"id":"jdCmkPXnM2wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VGG16Model(nn.Module):    # applying VGG16 model\n","    def __init__(self):\n","        super(VGG16Model, self).__init__()\n","        base_model = models.vgg16(pretrained=True)\n","\n","        self.conv = base_model.features\n","        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # set max pooling output by 7x7\n","\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512 * 7 * 7, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2)  # good or bad paper -> binary classification\n","        )\n","\n","        for param in self.conv.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.pool(x)\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"qF76XnmUM9Yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG16Model().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)  # low IR is recommended for VGG\n","\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    train_acc = 100 * correct / total\n","    print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\")"],"metadata":{"id":"yTN2qr_eM_hE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747652880578,"user_tz":-540,"elapsed":1846375,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"6485cf44-f6e9-4e55-d364-b8f70fdcc284"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1] Loss: 0.5868, Accuracy: 76.13%\n","[Epoch 2] Loss: 0.1329, Accuracy: 95.83%\n","[Epoch 3] Loss: 0.0533, Accuracy: 99.17%\n","[Epoch 4] Loss: 0.0330, Accuracy: 99.50%\n","[Epoch 5] Loss: 0.0282, Accuracy: 99.67%\n"]}]},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","test_acc = 100 * correct / total\n","print(f\"테스트 정확도: {test_acc:.2f}%\")"],"metadata":{"id":"NWsYOBOlNA5_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747588351576,"user_tz":-540,"elapsed":193539,"user":{"displayName":"룰라비","userId":"00641962345652782370"}},"outputId":"b5ac7220-cf3e-447b-e924-22ce40d60728"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["테스트 정확도: 83.59%\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/SAFE_AI(Project_proposal)/Models/ICLR_vgg_model.pt\")"],"metadata":{"id":"LAWmJdtjNCC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HijeG6rnZE4n"},"execution_count":null,"outputs":[]}]}